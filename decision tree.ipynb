{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e863c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d529fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data202.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c83edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dum2 = pd.get_dummies(df['plot'])\n",
    "dum3 = pd.get_dummies(df['mitti'])\n",
    "dum4 = pd.get_dummies(df['plant'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b21307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([df, dum2, dum3, dum4], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6126eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.drop(['plot', 'mitti', 'plant'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559417b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.fillna(final.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db3bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.replace([np.inf, -np.inf], np.finfo(np.float32).max, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7bf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "target_col = 'pro'\n",
    "final[target_col] = final[target_col].astype('category')\n",
    "final[target_col] = final[target_col].cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7b64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X = final.drop(target_col, axis=1)\n",
    "y = final[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3feae049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=10, min_samples_split=10, random_state=5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit decision tree model\n",
    "reg = DecisionTreeRegressor(criterion='squared_error', max_depth=10, min_samples_split=10, random_state=5)\n",
    "reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e272b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.7716023325362009\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "score = reg.score(X_test, y_test)\n",
    "print(\"R-squared:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc422f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7264488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT ON TEST SET\n",
    "y_pred=reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45f91d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 372.67909881286397\n"
     ]
    }
   ],
   "source": [
    "#CALCULATE MSE\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "print(\"MSE\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b4c8d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7844\\4293996516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Calculate the accuracy score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multilabel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m     95\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc08092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7716023325362009\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score (R-squared)\n",
    "from sklearn.metrics import r2_score\n",
    "accuracy = r2_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86805e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R-squared score\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abb3376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of features\n",
    "num_features = X_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b72ea323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples\n",
    "num_samples = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a983d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate adjusted R-squared score\n",
    "from sklearn.metrics import r2_score\n",
    "adjusted_r2 = 1 - ((1 - r2) * (num_samples - 1)) / (num_samples - num_features - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2694ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared: 0.574011792311705\n"
     ]
    }
   ],
   "source": [
    "print(\"Adjusted R-squared:\", adjusted_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef87d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When a model achieves the same accuracy and adjusted R2 score, it suggests that the model's performance is consistent across different evaluation metrics. This implies that the model is successfully capturing the underlying patterns and relationships in the data.\n",
    "\n",
    "The accuracy score measures the proportion of correct predictions made by the model, while the adjusted R2 score assesses the goodness of fit of the model by considering the proportion of variance explained, adjusted for the number of predictors.\n",
    "\n",
    "If both scores are the same, it indicates that the model is not overfitting or underfitting the data. It is achieving a good balance between capturing the patterns in the training data and generalizing well to unseen data.\n",
    "\n",
    "In summary, when the accuracy and adjusted R2 scores are the same, it suggests that the model is performing consistently and providing a reliable estimation of the relationship between the predictors and the target variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
